{
  
    
        "post0": {
            "title": "TrashNet",
            "content": "Background . Trash Recycling is a big problem in the United States alone. Every year, millinos of tons of trash is produced, a lot of which is recyclable. We&#39;re losing upwards of 75% of recyclables every year, which end up in a landfill rather than being recycled. There&#39;s a huge opportunity to minimize waste and reclaim the value of these recyclables. . CleanRobotics and TrashBot . TrashBot is a family of automated waste sorting bins produced by CleanRobotics. The idea behind TrashBot is to use the latest advances in AI and Computer Vision and Robotics to develop a product that can segregate trash more effectively than humans can. . CleanRobotics&#39;s current approach involves using single-class image classification to identify the trash items emptied into the TrashBot, and then use the underlying robotics to segregate it automatically. This leads to many problems, chief of which is not being able to do multi-class classification. You&#39;re also limited to only being to segregate one item at a time, severely impacting real world performance. . . Note: This was a consulting project for the AI Fellowship at Insight Data Science, NY in partnership with CleanRobotics. Express permission has been obtained for any data, figures and results being talked about here. Deployment is internal to CleanRobotics, and screenshots are used to demonstrate where necessary. . Dataset . As mentioned previously, the TrashBot currently works on an image classification model. The first task on the TODO list was to actually create a dataset suitable for object detection tasks. That meant going from per image label to per item bounding boxes and labels for items in each image in the dataset. I started by annotating the required ground truth boxes and class labels, creating an object detection dataset with roughly 8k images and upwards of 10k box annotations. . The dataset we&#39;re dealing with is highly imbalanced, and we&#39;ll go into data augmentation and training time transforms below. . Edge Deployment . In its current iteration, the network employed by the TrashBot captures the incoming data, processes it and sends it to the cloud for inference. This has proven to be a performance bottleneck, and the team is planning to move to edge inference to allow for faster decision making. The edge platform that CleanRobotics works on top of is the Google Coral, which is a hardware family built on top of EdgeTPUs designed by Google. . This brings alongwith it a set of challenges. First off, the model needs to be small enough to fit on the device itself. Secondly, the model needs to be TFLite and EdgeTPU compatible. . The EdgeTPU instruction set still doesn&#39;t support a lot of the core operations needed to implement a majority of the current state of the art in object detection. It&#39;s also restricted to TFLite, leaving Tensorflow as the framework of choice when it comes to development. . TrashNet . Now that you have a sense of the task at hand, let me introduce the work I&#39;ve done in this regard. . TrashNet is a collection of two models trained on the dataset described above. The first one is an EfficientDet, written in PyTorch, and the second one is an SSD MobileNet v2 model, trained using Tensorflow. . EfficientDet . EfficientDet is a family of object detection models that came out of Google Brain 1. It is based on the EfficientNet paper, which achieves best in class performance on the image classification task. EfficientDet has a scalable architecture while reducing the number of FLOPS by almost half and parameters by 10x. . . Note: EfficientDet no longer SoTA as of 3rd June 2020. . EfficientNet backbone . EfficientNet 2 aims to compound scale the network along multiple dimensions(width, depth and input resolution). Rather than only increasing one dimension at a time, EfficientNet expands the network along all dimensions. The authors use a compound scaling method to autmatically figure out the right scaling parameters, rather than manually turning the scaling coefficients. . Building from EfficientNet . EfficientDet builds on EfficientNet by adding a BiFPN(Bi-Directional Feature Pyramid Network) layer and a new compound scaling method to scale up the feature generation, resolution, backbone and the box/class prediction network. . BiFPN . A BiFPN aims to aggregate multi-scale features in a top-down manner. Conventional top-down FPNs are limited by the one way information flow. The authors of PANet added an extra bottom-up aggregation network. This comes at an extra computation cost. . The authors proposed further optimizations to make bi-directional feature fusion feasible. First, they remove nodes with a single input edge. Secondly, if the input and output node are at the same level, they added an extra edge from input to output node to fuse more features. Thirdly, they repeat the bidirectional path multiple times to allow for better feature fusion. The figure below presents the journey from an FPN to a BiFPN . . a. FPN top-down pathway for multi-scale feature fusion at levels 3-&gt;7 b. PANet additional bottom-up pathway building on from FPNs c. NAS-FPN Neural Architecture Search d. Expensive all to all feature generation e. Simplified PANet by removing nodes with single input edge f. BiFPN . For a review of Feature Pyramid Networks, check out this excellent introduction by Jonathan Hui on Medium . TrashNet - EfficientDet . This project only implements EfficientDet D0, foregoing compound scaling. After experimentation and trial runs using compound scaling, I found that EfficientDet D0 worked best. I hypothize this is because the dataset itself is small in size(roughly 8k images) and is highly imbalanced, leading to diminishing returns the more deeper the network got. . I use a pretrained EfficientNet backbone, trained on MS-COCO. This pretrained model is then used as the feature generator for the object detection task. The feature maps are then passed as inputs to the BiFPNs where they are fused together to learn multiple representations on the same input. . SSD MobileNet v2 . Single Shot Detector networks, as their name suggests detect objects in a single shot. They don&#39;t do any kind of region proposals, nor do they mess around with feature fusion or input scaling. . Single Shot Detectors take a single pass for feature extraction. After going through a certain number of convolutions for feature extraction, you get a fixed number of bounding boxes for each location. The number of convolutions is dependent on the backbone network used for feature extraction. . MobileNets are the backbone network here and are used for feature generation. The output of a MobileNet is a high dimensional feature map, that then gets piped to a SSD detector via a 3x3 convolution. . MobileNet v2, again by the folks at Google, uses inverted residual blocks(with strides 1 and 2). MobileNets use bottleneck inputs and outputs and lightweight depthwise convolutions without non-linearities to maintain representational power. . . MobileNet v2 3 also introduces shortcut connections between the bottleneck inputs, enabling faster training and better accuracy. Their high throughput and small footprint makes them especially suitable for devices with limited computing power. . As of today, SSD MobileNets are the only explicitly supported family of object detection models on the Google Coral making them a natural choice. For more information on models supported by Google Coral, click here. . Training . Class Imbalance . As mentioned above, the dataset I am working with is highly imbalanced. There are multiple ways to deal with class imbalance, the simplest and most difficult of which is getting more data. This is not always feasible and usually expensive. However, it is definitely possible to synthesize more data and that&#39;s the approach I&#39;ve taken. . After the dataset is built, a round of data augmentation is performed that uses image transforms such as flipping, scaling, rotation etc to generate different views of the ground truth data. I also go ahead and generate the ground truth bounding boxes necessary as we&#39;re dealing with an object detection problem. . Original Image Augmented Image Data Augmentation. HorizontalFlip, Translate(x=0.2, y=0.2) and RandomRotate(-2, 2) Pipeline . To deal with class imbalance, the original dataset was selectively augmented using the data augmentation described above. Classes with an average number of occurrences &lt; 200 were randomly augmented till they had at least 200 images. . For the EfficientDet network, the new augmented dataset was passed through the training loop for 50 epochs on an AWS p2.xlarge GPU instance, with each epoch taking around 20 minutes on average. The data augmentation loop is highly randomized, with different runs of augmentation yielding different versions of the dataset. . For the SSD MobileNet v2 network, the data was first converted to the TFRecords format. The TFRecords data was then passed as the input to a pretrained SSD MobileNet model from the Google Coral models repository. The network was then trained for about 100000 steps(about 120 epochs) on an AWS t2.2xlarge CPU instance. The Tensorflow model was then exported to a TFLite EdgeTPU compatible model using Coral board developer tools. . Note: Google Coral has an EdgeTPU architecture and hence, training on GPUs is not an option. . Visualizing Predictions . To visually inspect the predictions obtained from the model, I wrote a simple webapp using Streamlit. It took only a couple hundred lines of Python code, looks beautiful and is easily extensible allowing for all different kinds of summarized data presentation. The app is currently being tested interanally by the CleanRobotics team and is on track to be deployed in the early weeks of July. . Evaluation and Results . Learn More . GitHub | Documentation | Slides | Footnotes . 1. EfficientDet: Towards Scalable and Efficient Object Detection↩ . 2. EfficientNet: Improving Accuracy and Efficient through AutoML and Model Scaling↩ . 3. MobileNet v2↩ .",
            "url": "https://jsaurabh.dev/cv/object-detection/insight/2020/06/23/trashnet.html",
            "relUrl": "/cv/object-detection/insight/2020/06/23/trashnet.html",
            "date": " • Jun 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Recommender System using Content Filtering",
            "content": "Introduction . What are Recommendations Systems? Strictly speaking, Recommendations Systems (or RecSys as they are usually abbreviated) are a class of information filtering systems. Their goal is to improve the quality of results being delivered to the end user, by taking into account different heuristics related to the user itself. . Recommendation Systems power nearly every kind of content on the Internet, from search results on your favorite search engine to the Customers Who Bought This Item Also Bought window on Amazon.com. Netflix uses recommender systems to help you find the next show or movie to watch. They&#39;re everywhere and highly effective because of their ability to narrow down content for a user, be it a movie to watch or an item to buy. . There are primarily three different kinds of RecSys, each of which is formulated differently and has different operating characteristics. . Demographic | . Demographic(or popularity) based RecSys offer generalized recommendations. They offer non-personalized recommendations, instead focusing on what&#39;s universally popular(within the problem domain) and assuming that generality holds for all users from the target demography. They are simple and easy to get started with and don&#39;t require any kind of user data to be effective. That said, they&#39;re not as effective as others types of RecSys that we&#39;ll look at. . Content Based Filtering | . Classification based RecSys try to group together similar items. If a user has seen or shopped from an item, they&#39;re more likely to be interested in other entities from the same bucket. Instead of relying on user information, they rely on information about the item itself to group similar items together in the same bucket. More information can be found here . Collaborative Filtering | . Collaborative Filtering adds to the idea of Content Based Filtering by using similarities between user data and item data to make recommendations. To put it simply, consider the following scenario: person A has seen movie X and person B has seen movie Y. Now, it turns out person A and B have similar tastes. So the system will then recommend movie Y to A accounting for that similarity. More information can be found here on the Google Developers blog . TMDB5000 Dataset . TMDB5000 is a Kaggle hosted database derived from the TMDB API. You can learn more about TMDB on their website. It consists of roughly 5000 data entries, each of which has 20 features that we can work with. The data has probably gone through some amount of pre-processing steps, though there&#39;s a lot to be done, as we&#39;ll see below. . Let&#39;s dive into some of the code, so we can get started. . Note: This project was written as part of a challenge submission for the Insight Data Science Fellowship interview. As such, it focuses on breadth rather than depth. A more in depth post, building on this, will follow soon. . #collapse-hide #imports from utils import build_word_cloud, clean_num, get_month, get_day, get_director, get_list, clean_list, create_feature from pathlib import Path import warnings warnings.simplefilter(&#39;ignore&#39;) import ast import math import datetime import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.metrics.pairwise import cosine_similarity . . dataPath = Path(&#39;data/&#39;) . movies = pd.read_csv(dataPath/&#39;tmdb_5000_movies.csv&#39;) credits = pd.read_csv(dataPath/&#39;tmdb_5000_credits.csv&#39;) . movies.shape . (4803, 20) . movies.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4803 entries, 0 to 4802 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 budget 4803 non-null int64 1 genres 4803 non-null object 2 homepage 1712 non-null object 3 id 4803 non-null int64 4 keywords 4803 non-null object 5 original_language 4803 non-null object 6 original_title 4803 non-null object 7 overview 4800 non-null object 8 popularity 4803 non-null float64 9 production_companies 4803 non-null object 10 production_countries 4803 non-null object 11 release_date 4802 non-null object 12 revenue 4803 non-null int64 13 runtime 4801 non-null float64 14 spoken_languages 4803 non-null object 15 status 4803 non-null object 16 tagline 3959 non-null object 17 title 4803 non-null object 18 vote_average 4803 non-null float64 19 vote_count 4803 non-null int64 dtypes: float64(3), int64(4), object(13) memory usage: 750.6+ KB . We can see that we have data on 4803 different movies. The different features available include the budget, revenue of the movie, cast and crew as well as descriptive informatin about the genres and keywords. . Let&#39;s look at a few samples of the data, so we can get a sense of the work that we have to do before we can start extracting meaningful information. . movies.head(2) . budget genres homepage id keywords original_language original_title overview popularity production_companies production_countries release_date revenue runtime spoken_languages status tagline title vote_average vote_count . 0 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2009-12-10 | 2787965087 | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | . 1 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2007-05-19 | 961000000 | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | . Data Wrangling . As can be seen above, several features are in a stringified format, so we&#39;ll need to convert those back to their original formats. There&#39;s a homepage feature which has no information present for a majority of the data points, so removing that feature shouldn&#39;t lead to any major data loss. . Almost all of the features are represented as generic object dtypes, rather than native data types, so we&#39;ll look into that as well. We already have certain features, namely vote_count and vote_average that we can use to gain some kind of insight and build a baseline recommendation system. . Removing original_title feature . We have two features related to the title/name in the movie, namely title and original_title. There are however, a small number of cases when those two features aren&#39;t exactly the same (~6%) of the total dataset. A visual inspection tells us even in those cases, we can get the name of the movie by looking at title and original_language features. . Considering we&#39;re trying to build a recommender system, the name of the movie is going to be one of the lesser significant features and we can safely remove it from consideration. . cols = [&#39;title&#39;, &#39;original_title&#39;, &#39;original_language&#39;] movies[movies[&#39;original_title&#39;] != movies[&#39;title&#39;]][cols].head() . title original_title original_language . 97 Shin Godzilla | シン・ゴジラ | ja | . 215 Fantastic 4: Rise of the Silver Surfer | 4: Rise of the Silver Surfer | en | . 235 Asterix at the Olympic Games | Astérix aux Jeux Olympiques | fr | . 317 The Flowers of War | 金陵十三釵 | zh | . 474 Evolution | Évolution | fr | . movies = movies.drop(&#39;original_title&#39;, axis = 1) . Sanitizing the revenue feature . movies[&#39;revenue&#39;].describe() . count 4.803000e+03 mean 8.226064e+07 std 1.628571e+08 min 0.000000e+00 25% 0.000000e+00 50% 1.917000e+07 75% 9.291719e+07 max 2.787965e+09 Name: revenue, dtype: float64 . print(movies[movies[&#39;revenue&#39;] == 0].shape[0]/(movies.shape[0])) . 0.29710597543202166 . We can see above that roughly 30% of the entries in our data have no revenue ie revenue is 0. While it may come across as a not so useful feature, we can use it later. For now, let&#39;s sanitize the zero values so we get some info out of it. . movies[&#39;revenue&#39;] = movies[&#39;revenue&#39;].replace(0, math.nan) . Sanitizing the budget feature . movies[&#39;budget&#39;].describe() . count 4.803000e+03 mean 2.904504e+07 std 4.072239e+07 min 0.000000e+00 25% 7.900000e+05 50% 1.500000e+07 75% 4.000000e+07 max 3.800000e+08 Name: budget, dtype: float64 . print(movies[movies[&#39;budget&#39;] == 0].shape[0]/(movies.shape[0])) . 0.21590672496356444 . We can see above that roughly 22% of the entries in our data have no budget ie budget is 0. Let&#39;s sanitize the zero values, so we can use it later. . movies[&#39;budget&#39;] = movies[&#39;budget&#39;].replace(0, math.nan) . Sanitizing year feature . Let&#39;s sanitize the year column. Currently it&#39;s a generic object datatype, we&#39;ll convert it to a datetime representation . movies[&#39;year&#39;] = pd.to_datetime(movies[&#39;release_date&#39;], errors=&#39;coerce&#39;).apply( lambda x: str(x).split(&#39;-&#39;)[0] if x != math.nan else math.nan) . Create new feature: return . The feature return will describe the Return On Investment(ROI) for a movie. It&#39;s simply a numeric value that describes the revenue in terms of multiples from the original budget (investment). . movies[&#39;return&#39;] = movies[&#39;revenue&#39;] / movies[&#39;budget&#39;] . movies[&#39;return&#39;].describe() . count 3.229000e+03 mean 2.954822e+03 std 1.506101e+05 min 5.217391e-07 25% 1.022463e+00 50% 2.300366e+00 75% 4.420822e+00 max 8.500000e+06 Name: return, dtype: float64 . There&#39;s a lot more cleaning and aggregation that needs to be done. But without exploring the data first, and getting some kind of intuition for the data, we don&#39;t really know what to do. We&#39;ll explore the data first, and do any kind of cleaning and generating features on the fly later as needed. . EDA . Let&#39;s move on to do some exploratory data analysis, where we&#39;ll explore the dataset. . Word Clouds . We start by building world clouds, to get an idea of the titles and keywords for the dataset. . movies[&#39;title&#39;] = movies[&#39;title&#39;].astype(&#39;str&#39;) movies[&#39;overview&#39;] = movies[&#39;overview&#39;].astype(&#39;str&#39;) . build_word_cloud(movies, &#39;title&#39;) . build_word_cloud(movies, &#39;overview&#39;) . Languages . Let&#39;s look into the different languages of the movies available in the dataset. If I could take a guess, just from looking at the data, I&#39;d say most of the movies are in English, with a sprinking of some French, Japanese and Asian movies. . movies[&#39;original_language&#39;].drop_duplicates().shape . (37,) . There are 37 different languages, that&#39;s a wide range. Let&#39;s plot the occurence of each language . languages = pd.DataFrame(movies[&#39;original_language&#39;].value_counts()) languages[&#39;language&#39;] = languages.index columns = [&#39;count&#39;, &#39;language&#39;] languages.columns = columns . languages.head(7) . count language . en 4505 | en | . fr 70 | fr | . es 32 | es | . de 27 | de | . zh 27 | zh | . hi 19 | hi | . ja 16 | ja | . plt.figure(figsize=(10,5)) sns.barplot(x = &#39;language&#39;, y = &#39;count&#39;, data = languages) plt.show() . As expected, an overwhelmingly large number of the movies are in English. Let&#39;s look at a graph that makes it a bit easier for us to visualize the different languages present in the dataset. . plt.figure(figsize=(12,6)) sns.barplot(x = &#39;language&#39;, y = &#39;count&#39;, data = languages.iloc[1:]) plt.show() . Metrics . There are 3 fields in the dataset, which are hard metrics already given to us. They are popularity, vote_count and vote_average. . movies[&#39;popularity&#39;] = movies[&#39;popularity&#39;].apply(clean_num).astype(&#39;float&#39;) movies[&#39;vote_count&#39;] = movies[&#39;vote_count&#39;].apply(clean_num).astype(&#39;float&#39;) movies[&#39;vote_average&#39;] = movies[&#39;vote_average&#39;].apply(clean_num).astype(&#39;float&#39;) . #list movies by popularity movies[[&#39;title&#39;, &#39;popularity&#39;]].sort_values(&#39;popularity&#39;, ascending = False).head(5) . title popularity . 546 Minions | 875.581305 | . 95 Interstellar | 724.247784 | . 788 Deadpool | 514.569956 | . 94 Guardians of the Galaxy | 481.098624 | . 127 Mad Max: Fury Road | 434.278564 | . movies[[&#39;title&#39;, &#39;vote_count&#39;]].sort_values(&#39;vote_count&#39;, ascending = False).head(5) . title vote_count . 96 Inception | 13752 | . 65 The Dark Knight | 12002 | . 0 Avatar | 11800 | . 16 The Avengers | 11776 | . 788 Deadpool | 10995 | . movies[[&#39;title&#39;, &#39;vote_average&#39;]].sort_values(&#39;vote_average&#39;, ascending = False).head(5) . title vote_average . 3519 Stiff Upper Lips | 10.0 | . 4247 Me You and Five Bucks | 10.0 | . 4045 Dancer, Texas Pop. 81 | 10.0 | . 4662 Little Big Top | 10.0 | . 3992 Sardaarji | 9.5 | . Release Dates . Amongst all the features, we could be looking at or creating, perhaps few are as important as time of release. When a movie is released tends to have a strong correlation with how well it does. Major franchisee movies tend to be released around the time of holidays/summer months. Conversely, movies released around the time of holidays go on to do a lot better than those released around the year. Let&#39;s try to plot this distribution . movies[&#39;day&#39;] = movies[&#39;release_date&#39;].apply(get_day) movies[&#39;month&#39;] = movies[&#39;release_date&#39;].apply(get_month) . months = [&#39;Jan&#39;, &#39;Feb&#39;, &#39;Mar&#39;, &#39;Apr&#39;, &#39;May&#39;, &#39;Jun&#39;, &#39;Jul&#39;, &#39;Aug&#39;, &#39;Sep&#39;, &#39;Oct&#39;, &#39;Nov&#39;, &#39;Dec&#39;] sns.countplot(x = &#39;month&#39;, data = movies, order = months) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x196d5b41cc8&gt; . print(movies[&#39;revenue&#39;].mean()) . 117031352.91587678 . The mean revenue for movies in our dataset is 82260638.65167603. Let&#39;s now try to plot only the release dates of the movies whose revenue is greater than the mean for the data. . means = pd.DataFrame(movies[movies[&#39;revenue&#39;] &gt; 82260638.65167603].groupby(&#39;month&#39;)[&#39;revenue&#39;].mean()) means[&#39;month&#39;] = means.index plt.figure(figsize=(16,9)) plt.title(&#39;Average Revenue by Month&#39;) sns.barplot(x = &#39;month&#39;, y = &#39;revenue&#39;, data = means, order = months) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x196d5b1f608&gt; . From the chart above, we can see that the summer months(April - June) have the most number of movies who fare better than the numerically average movie in the dataset. . How similar/dissimilar is this chart from one where we graph all movies against the mean revenue? . means = pd.DataFrame(movies[movies[&#39;revenue&#39;] &gt; 1].groupby(&#39;month&#39;)[&#39;revenue&#39;].mean()) means[&#39;month&#39;] = means.index plt.figure(figsize=(16,9)) plt.title(&#39;Average Revenue by Month&#39;) sns.barplot(x = &#39;month&#39;, y = &#39;revenue&#39;, data = means, order = months) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x196ddb37588&gt; . Budget . Is there a correlation between the budget of a movie and its return? . plt.figure(figsize=(12,9)) sns.distplot(movies[movies[&#39;budget&#39;].notnull()][&#39;budget&#39;]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x196d5f05588&gt; . As we can see above, the budgets of the movies in the dataset are highly skewed , suggesting that a rather large number of movies have extremely small budgets . sns.jointplot(x = &#39;budget&#39;, y = &#39;revenue&#39;, data = movies) . &lt;seaborn.axisgrid.JointGrid at 0x196db62a588&gt; . The graph above suggests that there&#39;s a strong correlation between the budget of a movie and its revenue. . Revenue . movies[&#39;revenue&#39;].describe() . count 3.376000e+03 mean 1.170314e+08 std 1.834831e+08 min 5.000000e+00 25% 1.535290e+07 50% 5.175184e+07 75% 1.401651e+08 max 2.787965e+09 Name: revenue, dtype: float64 . best_revenue = movies[[&#39;title&#39;, &#39;budget&#39;, &#39;revenue&#39;]].sort_values(&#39;revenue&#39;, ascending = False) best_revenue . title budget revenue . 0 Avatar | 237000000.0 | 2.787965e+09 | . 25 Titanic | 200000000.0 | 1.845034e+09 | . 16 The Avengers | 220000000.0 | 1.519558e+09 | . 28 Jurassic World | 150000000.0 | 1.513529e+09 | . 44 Furious 7 | 190000000.0 | 1.506249e+09 | . ... ... | ... | ... | . 4797 Cavite | NaN | NaN | . 4799 Newlyweds | 9000.0 | NaN | . 4800 Signed, Sealed, Delivered | NaN | NaN | . 4801 Shanghai Calling | NaN | NaN | . 4802 My Date with Drew | NaN | NaN | . 4803 rows × 3 columns . A useful analysis that needs to be done here is to take inflation into account. The movies at the top are all from recent times, so maybe inflation analysis is something that&#39;s necessary to get meaningful information from this feature. . Genres . #convert string-ified lists to list representation movies[&#39;genres&#39;] = movies[&#39;genres&#39;].fillna(&#39;[]&#39;).apply( ast.literal_eval).apply(lambda x: [i[&#39;name&#39;] for i in x] if isinstance(x, list) else []) . series = movies.apply(lambda x: pd.Series(x[&#39;genres&#39;]),axis = 1).stack().reset_index(level = 1, drop = True) series.name = &#39;genres&#39; genres = movies.drop(&#39;genres&#39;, axis = 1).join(series) genres[&#39;genres&#39;].value_counts().shape[0] . 20 . There are 20 different genres, which we look at below . popular = pd.DataFrame(genres[&#39;genres&#39;].value_counts()).reset_index() popular.columns = [&#39;genre&#39;, &#39;count&#39;] popular.head() . genre count . 0 Drama | 2297 | . 1 Comedy | 1722 | . 2 Thriller | 1274 | . 3 Action | 1154 | . 4 Romance | 894 | . plt.figure(figsize = (18,9)) sns.barplot(x = &#39;genre&#39;, y = &#39;count&#39;, data = popular) plt.show() . Let&#39;s look at the trends over time for a specific set of genres . genre = [&#39;Drama&#39;, &#39;Action&#39;, &#39;Comedy&#39;, &#39;Thriller&#39;, &#39;Romance&#39;, &#39;Adventure&#39;, &#39;Horror&#39;, &#39;Family&#39;] . genres[&#39;year&#39;] = genres[&#39;year&#39;].replace(&#39;NaT&#39;, math.nan) genres[&#39;year&#39;] = genres[&#39;year&#39;].apply(clean_num) . trends = genres[(genres[&#39;genres&#39;].isin(genre)) &amp; (genres[&#39;year&#39;] &gt;= 2000) &amp; (genres[&#39;year&#39;] &lt;= 2016)] ctab = pd.crosstab([trends[&#39;year&#39;]], trends[&#39;genres&#39;]).apply(lambda x: x/x.sum(), axis = 1) ctab[genre].plot(kind = &#39;line&#39;, stacked = False, colormap = &#39;jet&#39;, figsize = (18,9)) plt.legend(bbox_to_anchor=(1, 1)) . &lt;matplotlib.legend.Legend at 0x196d65dca08&gt; . There seems to be a sharp decline in the number of drama movies from 2014-2016 while the number of movies under horror and action has gone up around the same time. . Modeling . There&#39;s a lot of interesting possibilities in terms of the viz that we could have done. Ideally, we will clean and analyze every possible feature and try to identify its importance towards the final task. However, for the purposes of keeping this clean and simple, I&#39;ll go ahead and build what we&#39;re actually here for: a recommendation system. . We&#39;ll be building a Content Filtering based RecSys. In a way, given the dataset, we are limited in terms of the approach we can take. We don&#39;t have user information available in the data, so collaborative filtering is unfortunately not an option. We do have metadata about the movies itself, and that fits in perfectly within the operating characteristics of a Content Filtering based recommendation system. . movies = pd.read_csv(dataPath/&#39;tmdb_5000_movies.csv&#39;) credits = pd.read_csv(dataPath/&#39;tmdb_5000_credits.csv&#39;) . credits.head(2) . movie_id title cast crew . 0 19995 | Avatar | [{&quot;cast_id&quot;: 242, &quot;character&quot;: &quot;Jake Sully&quot;, &quot;... | [{&quot;credit_id&quot;: &quot;52fe48009251416c750aca23&quot;, &quot;de... | . 1 285 | Pirates of the Caribbean: At World&#39;s End | [{&quot;cast_id&quot;: 4, &quot;character&quot;: &quot;Captain Jack Spa... | [{&quot;credit_id&quot;: &quot;52fe4232c3a36847f800b579&quot;, &quot;de... | . movies.head(2) . budget genres homepage id keywords original_language original_title overview popularity production_companies production_countries release_date revenue runtime spoken_languages status tagline title vote_average vote_count . 0 237000000 | [{&quot;id&quot;: 28, &quot;name&quot;: &quot;Action&quot;}, {&quot;id&quot;: 12, &quot;nam... | http://www.avatarmovie.com/ | 19995 | [{&quot;id&quot;: 1463, &quot;name&quot;: &quot;culture clash&quot;}, {&quot;id&quot;:... | en | Avatar | In the 22nd century, a paraplegic Marine is di... | 150.437577 | [{&quot;name&quot;: &quot;Ingenious Film Partners&quot;, &quot;id&quot;: 289... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2009-12-10 | 2787965087 | 162.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}, {&quot;iso... | Released | Enter the World of Pandora. | Avatar | 7.2 | 11800 | . 1 300000000 | [{&quot;id&quot;: 12, &quot;name&quot;: &quot;Adventure&quot;}, {&quot;id&quot;: 14, &quot;... | http://disney.go.com/disneypictures/pirates/ | 285 | [{&quot;id&quot;: 270, &quot;name&quot;: &quot;ocean&quot;}, {&quot;id&quot;: 726, &quot;na... | en | Pirates of the Caribbean: At World&#39;s End | Captain Barbossa, long believed to be dead, ha... | 139.082615 | [{&quot;name&quot;: &quot;Walt Disney Pictures&quot;, &quot;id&quot;: 2}, {&quot;... | [{&quot;iso_3166_1&quot;: &quot;US&quot;, &quot;name&quot;: &quot;United States o... | 2007-05-19 | 961000000 | 169.0 | [{&quot;iso_639_1&quot;: &quot;en&quot;, &quot;name&quot;: &quot;English&quot;}] | Released | At the end of the world, the adventure begins. | Pirates of the Caribbean: At World&#39;s End | 6.9 | 4500 | . Join both the DataFrames on the id attribute, so we can incorporate features from both datasets. . credits.columns = [&#39;id&#39;, &#39;title&#39;, &#39;cast&#39;, &#39;crew&#39;] data = movies.merge(credits, on = &#39;id&#39;) . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 4803 entries, 0 to 4802 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 budget 4803 non-null int64 1 genres 4803 non-null object 2 homepage 1712 non-null object 3 id 4803 non-null int64 4 keywords 4803 non-null object 5 original_language 4803 non-null object 6 original_title 4803 non-null object 7 overview 4800 non-null object 8 popularity 4803 non-null float64 9 production_companies 4803 non-null object 10 production_countries 4803 non-null object 11 release_date 4802 non-null object 12 revenue 4803 non-null int64 13 runtime 4801 non-null float64 14 spoken_languages 4803 non-null object 15 status 4803 non-null object 16 tagline 3959 non-null object 17 title_x 4803 non-null object 18 vote_average 4803 non-null float64 19 vote_count 4803 non-null int64 20 title_y 4803 non-null object 21 cast 4803 non-null object 22 crew 4803 non-null object dtypes: float64(3), int64(4), object(16) memory usage: 900.6+ KB . Recommendations based on Movie Blurb . In the given data, the overview is a short blurb for the movie. We&#39;ll use this info to find other movies which have a similar description. . data[&#39;overview&#39;] = data[&#39;overview&#39;].fillna(&#39;&#39;) overview = data[&#39;overview&#39;] . We&#39;ll convert each word in the overview to a word vector, so we can assign importance to each vector based on the number of occurences. This can be done using the TF-IDF Vectorizer. For more information, check out this blog . tfidf = TfidfVectorizer(stop_words = &#39;english&#39;) mat = tfidf.fit_transform(overview) . Here, mat is the matrix obtained from the TfidfVectorizer. The shape of the matrix indicates that there are 4803 movies, with a total of 20978 words being used to describe all the movies. For intuition, the matrix obtained is going to be a sparse matrix, with the output format as follow: . (document_id, token_id) score . cosine_sim = cosine_similarity(mat, mat) . Now that we&#39;ve obtained our matrix, next we need to determine, using the TF-IDF scores, which two movies are alike. There are several ways we can try to obtain this similarity, one of which is cosine similarity. . Simply put, cosine similarity is the measure of distance between 2 vectors. It measure the cosine of the angle between the 2 vectors. The smaller the angle is, the more similar the 2 vectors are. It follows from trignometry that cos(0) is 1 and cos(90) is 0 ie. vectors that are parallel to each other are likely to be similar and vectors which are orthogonal are likely to be dissimilar. . Next, we&#39;ll do reverse indexing, so that if we are given a string representing a movie, we can get its id and index it in our dataset. . title2id = pd.Series(data.index, index = data[&#39;title_x&#39;]) title2id.shape . (4803,) . def recommend(title, measure, npreds): idx = title2id[title] score = list(enumerate(measure[idx])) score = sorted(score, key = lambda x: x[1], reverse = True) score = score[:npreds] idxs = [i[0] for i in score] return data[&#39;title_x&#39;].iloc[idxs] . recommend(&quot;Pirates of the Caribbean: At World&#39;s End&quot;, cosine_sim, 10) . 1 Pirates of the Caribbean: At World&#39;s End 2542 What&#39;s Love Got to Do with It 3095 My Blueberry Nights 2102 The Descendants 1280 Disturbia 3632 90 Minutes in Heaven 792 Just Like Heaven 1709 Space Pirate Captain Harlock 1799 Original Sin 2652 Bathory: Countess of Blood Name: title_x, dtype: object . recommend(&quot;El Mariachi&quot;, cosine_sim, 10) . 4798 El Mariachi 1701 Once Upon a Time in Mexico 3959 My Big Fat Independent Movie 3704 Salvador 4769 The Legend of God&#39;s Gun 729 A Civil Action 1965 Footloose 324 The Road to El Dorado 3853 2:13 421 Zodiac Name: title_x, dtype: object . recommend(&#39;The Dark Knight Rises&#39;, cosine_sim, 10) . 3 The Dark Knight Rises 65 The Dark Knight 299 Batman Forever 428 Batman Returns 1359 Batman 3854 Batman: The Dark Knight Returns, Part 2 119 Batman Begins 2507 Slow Burn 9 Batman v Superman: Dawn of Justice 1181 JFK Name: title_x, dtype: object . Recommendations based on metadata . We can also use the metadata of a given movie to get more relevant recommendations. For now, we&#39;ll focus on the cast and crew, genres and keyword features to generate recommendations which are similar to the input. . #parse stringified objects pd.set_option(&#39;display.max_colwidth&#39;, 100) features = [&#39;keywords&#39;, &#39;genres&#39;, &#39;cast&#39;, &#39;crew&#39;] for feature in features: data[feature] = data[feature].apply(ast.literal_eval) . data[&#39;director&#39;] = data[&#39;crew&#39;].apply(get_director) . data[[&#39;director&#39;, &#39;title_x&#39;]].head(5) . director title_x . 0 James Cameron | Avatar | . 1 Gore Verbinski | Pirates of the Caribbean: At World&#39;s End | . 2 Sam Mendes | Spectre | . 3 Christopher Nolan | The Dark Knight Rises | . 4 Andrew Stanton | John Carter | . features = [&#39;cast&#39;, &#39;keywords&#39;, &#39;genres&#39;] for feature in features: data[feature] = data[feature].apply(get_list) . cols = [&#39;title_x&#39;, &#39;director&#39;, &#39;cast&#39;, &#39;keywords&#39;, &#39;genres&#39;] data[cols].head(3) . title_x director cast keywords genres . 0 Avatar | James Cameron | [Sam Worthington, Zoe Saldana, Sigourney Weaver, Stephen Lang, Michelle Rodriguez] | [culture clash, future, space war, space colony, society] | [Action, Adventure, Fantasy, Science Fiction] | . 1 Pirates of the Caribbean: At World&#39;s End | Gore Verbinski | [Johnny Depp, Orlando Bloom, Keira Knightley, Stellan Skarsgård, Chow Yun-fat] | [ocean, drug abuse, exotic island, east india trading company, love of one&#39;s life] | [Adventure, Fantasy, Action] | . 2 Spectre | Sam Mendes | [Daniel Craig, Christoph Waltz, Léa Seydoux, Ralph Fiennes, Monica Bellucci] | [spy, based on novel, secret agent, sequel, mi6] | [Action, Adventure, Crime] | . for feature in features: data[feature] = data[feature].apply(clean_list) . data[cols].head(3) . title_x director cast keywords genres . 0 Avatar | James Cameron | [samworthington, zoesaldana, sigourneyweaver, stephenlang, michellerodriguez] | [cultureclash, future, spacewar, spacecolony, society] | [action, adventure, fantasy, sciencefiction] | . 1 Pirates of the Caribbean: At World&#39;s End | Gore Verbinski | [johnnydepp, orlandobloom, keiraknightley, stellanskarsgård, chowyun-fat] | [ocean, drugabuse, exoticisland, eastindiatradingcompany, loveofone&#39;slife] | [adventure, fantasy, action] | . 2 Spectre | Sam Mendes | [danielcraig, christophwaltz, léaseydoux, ralphfiennes, monicabellucci] | [spy, basedonnovel, secretagent, sequel, mi6] | [action, adventure, crime] | . data[&#39;feature&#39;] = data.apply(create_feature, axis = 1) cols = [&#39;title_x&#39;, &#39;director&#39;, &#39;cast&#39;, &#39;keywords&#39;, &#39;genres&#39;, &#39;feature&#39;] data[cols].head(3) . title_x director cast keywords genres feature . 0 Avatar | James Cameron | [samworthington, zoesaldana, sigourneyweaver, stephenlang, michellerodriguez] | [cultureclash, future, spacewar, spacecolony, society] | [action, adventure, fantasy, sciencefiction] | cultureclash future spacewar spacecolony society samworthington zoesaldana sigourneyweaver steph... | . 1 Pirates of the Caribbean: At World&#39;s End | Gore Verbinski | [johnnydepp, orlandobloom, keiraknightley, stellanskarsgård, chowyun-fat] | [ocean, drugabuse, exoticisland, eastindiatradingcompany, loveofone&#39;slife] | [adventure, fantasy, action] | ocean drugabuse exoticisland eastindiatradingcompany loveofone&#39;slife johnnydepp orlandobloom kei... | . 2 Spectre | Sam Mendes | [danielcraig, christophwaltz, léaseydoux, ralphfiennes, monicabellucci] | [spy, basedonnovel, secretagent, sequel, mi6] | [action, adventure, crime] | spy basedonnovel secretagent sequel mi6 danielcraig christophwaltz léaseydoux ralphfiennes monic... | . count = CountVectorizer(stop_words = &#39;english&#39;) count_mat = count.fit_transform(data[&#39;feature&#39;]) . cosine_sim1 = cosine_similarity(count_mat, count_mat) . data = data.reset_index() idxs = pd.Series(data.index, index = data[&#39;title_x&#39;]) . recommend(&quot;Pirates of the Caribbean: At World&#39;s End&quot;, cosine_sim1, 10) . 1 Pirates of the Caribbean: At World&#39;s End 12 Pirates of the Caribbean: Dead Man&#39;s Chest 199 Pirates of the Caribbean: The Curse of the Black Pearl 13 The Lone Ranger 5 Spider-Man 3 30 Spider-Man 2 1652 Dragonball Evolution 17 Pirates of the Caribbean: On Stranger Tides 115 Hancock 129 Thor Name: title_x, dtype: object . recommend(&quot;El Mariachi&quot;, cosine_sim1, 10) . 4798 El Mariachi 1177 Sin City 2229 Machete Kills 3349 Desperado 2181 From Dusk Till Dawn 880 Grindhouse 669 Sin City: A Dame to Kill For 856 Turbulence 2803 Machete 1829 No Country for Old Men Name: title_x, dtype: object . recommend(&#39;The Dark Knight Rises&#39;, cosine_sim1, 10) . 3 The Dark Knight Rises 119 Batman Begins 65 The Dark Knight 1196 The Prestige 4638 Amidst the Devil&#39;s Wings 3332 Harry Brown 4099 Harsh Times 95 Interstellar 1178 Vantage Point 2398 Hitman Name: title_x, dtype: object .",
            "url": "https://jsaurabh.dev/ml/recsys/pandas/sklearn/2020/04/29/tmdb5000.html",
            "relUrl": "/ml/recsys/pandas/sklearn/2020/04/29/tmdb5000.html",
            "date": " • Apr 29, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Markov Localization",
            "content": "",
            "url": "https://jsaurabh.dev/robotics/localization/sdc/2020/04/25/markov-localization.html",
            "relUrl": "/robotics/localization/sdc/2020/04/25/markov-localization.html",
            "date": " • Apr 25, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Localization for Autonomous Vehicles",
            "content": "Localization . Localization, as you have probably guessed, is the process of locating oneself, usually in the confines of an environment. In the context of autonomous vehicles, localization is an extremely important step. It is what enables self driving cars to function properly and make proper turns, rather than crashing head first into a traffic signal or a tree or any other kind of obstacle that may be in its path. . As the name implies, to localize is to know where you are. Historically, Global Positioning Systems(GPS) has been used for this very purpose. However, as important and useful GPS is, it&#39;s simply not accurate enough to be useful in critical systems such as self driving cars. It is essential that a self driving car be accurate in its measurements and have confidence in that accuracy, or the alternative is tons of crashes and accidents. . GPS has upto 2-10 meters of error. That&#39;s not ideal, and the error needs to be better, in cm rather than meters That&#39;s the question that the localization process tries to answer. Let&#39;s get into the intution of how that actually happens, and then we&#39;ll implement a set of functions in python that are essentially how localization for self driving car works. . &gt; Important: The type of localization I&#39;m writing about needs a good high definition map of the environment that the car is going to be operating in. Simultaneous Localization and Mapping(SLAM) is another type of localization that doesn&#39;t suffer from the same constraint . Intuition . Localization really is about continuous sensing and movement. You start out with a prior belief, which represents your view of what the environment looks like. On the basis of the belief, we sense the environment, update our belief based on the sensing results and make movements that correspond to the new information we learned from sensing. . In a perfect world, robot motion would be exact. However, it hardly ever is. Sensing itself is not perfect, and there&#39;s always going to be some kind of error involved, either in your movements or in the precision of your sensing. But intuitively, accounting for imperfect sensing and motion, it follows that we gain information when we sense(that information may not be highly accurate, but it&#39;s a gain). And similarly, when we move with inexact motion, we lose information. That&#39;s what localization really is, a continuous cycle of sensing and movement. And the imperfectness of the motion and sensing is what makes it a hard problem to solve. . Let&#39;s try to understand, using code, these two processes. . Sensing . Sensing takes in a belief(or a prior distribution) for the environment, and then updates that belief after sensing the environment. Say you have a bunch of buckets, either green or red in a uniform probability distribution. You go on and sense, say red buckets. It follows from probability theory that the belief for the red buckets should go up, and correspondingly the belief for the green buckets should go down. . Let&#39;s implement this in code. . #collapse-show import random n_buckets = 5 buckets = [random.choice([&#39;green&#39;, &#39;red&#39;]) for _ in range(n_buckets)] p = [1./n_buckets] * n_buckets # uniform probability distribution print(buckets) print(p) . . [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;] [0.2, 0.2, 0.2, 0.2, 0.2] . This defines the environment and our prior belief. Now, let&#39;s see what happens when we sense. Say we sense a red bucket. How does this affect my prior belief over different buckets? . The probabilities for the red buckets should go up, and the ones for green buckets should go down. Any bucket for which the color and our sensing is correct, we multipy it by a large weight, and a smaller weight for which it&#39;s incorrect. . #collapse-show pHit = 0.6 #for correct measurements pMiss = 0.2 #for incorrect measurements for i in range(n_buckets): if buckets[i] == &#39;green&#39;: p[i] *= pMiss else: p[i] *= pHit print(p) . . [0.12, 0.12, 0.04000000000000001, 0.04000000000000001, 0.04000000000000001] . This represents our new distribution, also called a posterior probability distribution. However, as you can see it&#39;s not a valid probability distribution as it doesn&#39;t sum to 1. To fix that, we&#39;ll normalize it . #collapse-show s = sum(p) for i in range(n_buckets): p[i] /= s print(p) . . [0.3333333333333332, 0.3333333333333332, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111] . As you can see, this is a valid distribution. This is what sensing does. If you compare our posterior distribution with our prior belief, we can see that we have more information and have a greater idea about the environment for the given task(namely sensing red buckets). This is the information gain described earlier. . Let&#39;s refactor everything into one function. . def sense(p, t): res = [] for idx, _ in enumerate(p): hit = (buckets[idx] == t) res.append(p[idx] * (hit * pHit + (1-hit) * pMiss)) s = sum(res) for i in range(len(res)): res[i] /= s return res n_buckets = 5 buckets = [random.choice([&#39;green&#39;, &#39;red&#39;]) for _ in range(n_buckets)] p = [1./n_buckets] * n_buckets #prior print(buckets) print(&quot;Prior distribution is: {0}&quot;.format(p)) target = &#39;red&#39; p = sense(p, target) #posterior print(&quot;Posterior distribution is: {0}&quot;.format(p)) . [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;red&#39;] Prior distribution is: [0.2, 0.2, 0.2, 0.2, 0.2] Posterior distribution is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] . Movement . Movement is the second part of the localization equation. We&#39;ll only deal with inexact motion, exact motion is a trivial case and a rare scenarior in real world performance. . Assuming the world is cyclic, that is the rightmost bucket wraps around as the new leftmost bucket as you move them around. Inexact motion exists when there is uncertainty about the accuracy of the motion of a robot. . Let&#39;s define the parameters for the inexact motion. Say the probabilties are as follows: . Exact Motion : $P(X_{i+t} mid X_{i})$ = 0.8 | Undershooting: $P(X_{i+t-1} mid X_{i})$ = 0.1 | Overshooting : $P(X_{i+t+1} mid X_{i})$ = 0.1 where t is the target | . What this means is that we have an 80% chance of an exact motion and a 10% chance of either undershooting or overshooting our target. Assume that overshooting or undershooting happens in incrmenents(decrements) of 1. . Let&#39;s define inexact motion, accounting for the probabilities defined above. . &gt; Note: For uniform distributions, inexact motion has no effect on the posterior distribution. Uniform distributions are the state of least information. As everything is equally likely, we know nothing extra. . def move(p, unit): q = [] for i in range(len(p)): s = pExact * p[(i-unit% len(p))] s += pOvershoot * p[(i+1-unit) % len(p)] s += pUndershoot * p[(i-1-unit) % len(p)] q.append(s) return q pExact = 0.8 pOvershoot, pUndershoot = 0.1, 0.1 print(&quot;Environment is: {0}&quot;.format(buckets)) print(&quot;Prior distribution is: {0}&quot;.format(p)) print(&quot;Distribution after movement is: {0}&quot;.format(move(p, 1))) . Environment is: [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;red&#39;] Prior distribution is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] Distribution after movement is: [0.2545454545454545, 0.2727272727272727, 0.2545454545454545, 0.1090909090909091, 0.1090909090909091] . It&#39;s not easily seen, but given our target is red buckets, the distribution after movements suggests that we now know less about the presence of red buckets in the environment. What would happen if we were to move again, say 2 times? . #collapse-show print(&quot;Prior distribution(after sensing) is: {0}&quot;.format(p)) for i in range(2): p = move(p, 1) print(&quot;Posterior distribution(after movement) is: {0}&quot;.format(p)) . . Prior distribution(after sensing) is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] Posterior distribution(after movement) is: [0.12363636363636364, 0.24181818181818182, 0.26909090909090905, 0.24181818181818182, 0.12363636363636364] . The uncertainty keeps increasing, and the amount of information gain keeps going down(wrt to the target). This is the entire localization loop, a continuous iteration of sense and move. You take an initial belief, use it to sense and then move accordingly. Every iteration of this loop(as we&#39;ve just shown) cumulatively causes information loss, largely because of the inexact motion of robots. . Every time we sense, the probability distribution is more focused ie. it learns more and after every iteration of move, the distribution is a bit more spread out because of information loss. . Formalizing Localization . We looked at a very simple example above. While it may not look like we&#39;ve implemented localization, it does implement the essence of how localization works. In the real world, the green and red buckets would be substituted by lane markers, trees, signposts etc. And the measurements obtained from sensing(say a camera) would be the color of those individual objects. This same approach can be used to work on image data, as long as we can make the proper associations. . Sensing . $P(X mid Z) = frac {P(Z mid X) * P(X)}{P(Z)}$ . The above equation defines the Bayes rule, which incorporates prior knowledge to find the probability of an event occuring. . In terms of measurements(sensing), consider X to be one of the grids in a map and Z to be the measurement update. So, the equation described above intends to calculate the probability of the location after taking the measurement into consideration . Here, $P(X)$ is the prior distribution and $P(X mid Z) is the probability of a colored bucket for every possible location in the environment. The product of these will correspond to the non-normalized probability distribution from earlier. . It then follows that $P(Z)$ is the sum of such probability distributions, over all grid cells in the environment leading to a normalized posterior distribution. What $P(Z)$ is really doing is assigning each cell a probability, irrespective of which cell it is. . 1. Referenced from the Localization module in the Udacity Self Driving Cars Nanodegree program↩ .",
            "url": "https://jsaurabh.dev/robotics/localization/sdc/2020/04/21/localization.html",
            "relUrl": "/robotics/localization/sdc/2020/04/21/localization.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Machine Learning Pipelines - Part II",
            "content": "Machine Learning Pipelines - Part II . In Part I, we went ahead and wrote a rudimentary version of the pipeline we’ll be using for the webapp. . . It&#39;s not a pipeline In this post, we’ll focus on acquiring an initial dataset and extract and generate features. . Writing Better Questions . Following along with the book, we want to build an editor that lets its users write better questions. Before we go and build a model, the first step is playing around with the data. That begs the question, what kind of dataset should we be looking at? . Some good places to find datasets are Kaggle, the UCI Machine Learning Repository as well as the AWS Open Data registry. . For our use case, we’ll go ahead with StackExchange dump specifically the Writers dump here . Each of these dumps is an XML file with the headers and attributes containing the actual info we need. We need to extract raw text, and this is where we’ll write a basic pipeline to get the data we need. .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "relUrl": "/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Machine Learning Pipelines - Part I",
            "content": "Machine Learning Pipelines - Part I . I recently started reading Building Machine Learning Powered Applications by Emmanuel Ammeisen. What little I’ve read so far has been excellent, and I love the approach of building a real application as we walk through the book. It’s just what I’ve been looking for and I can’t wait to apply what I learn to my projects and try to extend them from notebooks to production ready web apps, however miniature my operating scale is. . For now, this post is just me walking through the exact use case as the book(build an assistant to help users write better questions). Once I’m done with that, I plan to go back and follow the steps in the book for a real application of my own. What that is, I don’t know yet but for now - let’s dive into pipelines. . Pipelines . What are pipelines? In software engineering terms, pipelines are a series of processes on inputs in a sequential manner ie. output of one becomes the input to the other. For machine learning, this usually reduces to the data transformation and pre-processing that needs to be done before the data is suitable to be fed into an architecture. As ML has exploded, so has the research and time spent on developing scalable infrastructure. The field is in a constant state of flux, with new libraries and open source tools being released rapidly. Microsoft Azure has a nice overview on the different kinds of workflows involved in an ML pipeline [here].(https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines) . Our use case . For now, let’s continue with a simple pipeline of our own. We’ll assume that our model is already trained and focus on the inference/serving pipeline. An inference pipeline takes in the user input, processes it, passes it to the model and presents the user with the results. . Accept user input . We can build a simple web app with a textbox to allow the user to enter text input. For this, we’ll look into fastAPI which as the name suggests, is a web framework with high performance. For more info on fastAPI, the documentation has an excellent tutorial. . Let’s start by defining the .html for the text input web app -:p . &lt;html&gt; &lt;body&gt; &lt;form name=&quot;form1&quot; method=&quot;get&quot; action=&quot;/input&quot; enctype=&quot;application/json&quot;&gt; &lt;div&gt; &lt;h1&gt;Write better questions&lt;/h1&gt; &lt;input id=&quot;search&quot; name=&quot;search&quot; type=&quot;text&quot; /&gt; &lt;br&gt;&lt;br&gt; &lt;div&gt; &lt;input type=&quot;submit&quot; value=&quot;Search&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; &lt;/html&gt; . This will give us a simple text input box, followed by a search button. It should look as follows: . Text input box Next up, we’ll define the logic needed to handle the incoming input using fastAPI. I know it’s overkill but it’s a chance for me to learn and dive deep into fastAPI which is something I’ve wanted to get into for a while. . from fastapi import FastAPI from starlette.templating import Jinja2Templates app = FastAPI() templates = Jinja2Templates(directory=&#39;templates&#39;) @app.route(&quot;/&quot;) def home(request): context = {&quot;request&quot;: request} return templates.TemplateResponse(&quot;index.html&quot;, context) @app.get(&quot;/input/&quot;) def read_input(search: str): return search . The lines of code above are straight from the fastAPI documentation. The index.html template that we defined above gets served at the root address (in this case localhost on port 8000) and the second function takes in the entered text as a query parameter and returns it. Trivial yes, but it defines the workflow that we are going to be using. . Process User Input . Now that we’ve accepted in a string, let’s go and process it. Pre-processing steps for textual data usually involve removing puncutation, converting to lowercase, removing stop words, white spaces etc. There’s a lot of room here, and the preprocessing involved will be dictated by the task and data at hand. Let’s dive into the pre-processing. . def lower(text: str) -&gt; str: return text.lower() def remove_punctuation(text: str) -&gt; str: return text.translate(str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation)) def sanitize_ascii(text: str) -&gt; str: return text.encode(encoding=&quot;ascii&quot;, errors=&quot;ignore&quot;).decode() . Starting out, we can assume that this level of pre-processing is enough. Later on, we’ll come back and go through all the steps required for serving such an app in production. . Tokenize . For tokenizing our input, we’ll leverage the spaCy tokenizer. We’ll also need to download an English language pretrained model to actually give us the tokens we need as the library by itself does not come with one built-in. You can refer the documentation here . import spacy as sp spacy = sp.load(&quot;en_core_web_sm&quot;) def tokenize(text: str) -&gt; List[str]: return [token.text for token in spacy(text)] . Feature Generation . Now that we’ve defined our pre-processing strategy, the next step is to generate features. Features, as the name suggests are indicators that let us learn something meaningful about the data. . We’ll focus on the Flesch reading-ease score . We assume that sentences only end with periods(.) and the input is in the English language. Let’s compute the individual stats for the Flesch reading-ease score: . def count_syllables(text: str) -&gt; int: ### https://codereview.stackexchange.com/a/224180 return len( re.findall(&#39;(?!e$)[aeiouy]+&#39;, text, re.I) + re.findall(&#39;^[^aeiouy]*e$&#39;, text, re.I) ) def count_words(text: str) -&gt; int: return len(re.findall(r&#39; w+&#39;, text)) def count_sentences(text: str) -&gt; int: return len(re.split(r&#39;.+&#39;, text)) . The Flesch score can then be easily computed as follows: . def flesch_score(text: str) -&gt; float: total_syllables = count_syllables(text) total_words = count_words(text) total_sentences = count_sentences(text) return 206.835 - 1.015 * float(total_words)/total_sentences - 84.6 * (float(total_syllables)/total_words) . That’s it for this post. I’ll pick up with Feature Extraction for the next post. . Any questions/comments or suggestions, please let me know in the comments below! .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/02/26/ml-pipelines.html",
            "relUrl": "/ml/mlpa/2020/02/26/ml-pipelines.html",
            "date": " • Feb 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Computer Science Masters grad from SUNY Buffalo, with a specialization in Artificial Intelligence. During my time at UB, I worked on autonomous vehicles for the iCAVE2 project as well as swarm robotics for the DARPA OFFSET Program grant. . I also ran the Deep Learning student community at UB, where we held workshops, tutorials, hackathons and talks on topics as diverse as language modeling to graph neural networks. Most recently, I was awarded the Graduate Entrepreneurship award for building consumer focused products while a graduate student in the Department of Computer Science. . Outside of code, I like building and takings things apart(a screwdriver toolkit was the first thing I ever bought), and have been known to enjoy the odd Star Wars movie marathon. .",
          "url": "https://jsaurabh.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
      ,"page7": {
          "title": "TrashNet",
          "content": "TrashNet is a project that I worked on in collaboration with CleanRobotics as part of the AI Fellowship at Insight Data Science1, New York. . TrashNet takes a multi-pronged approach to identifying trash, building two classes of models. One of these(EfficientDet) is designed for throughput, enabling parallel inference. The other class of models, using the SSD MobileNet v2 architecture is designed for edge devices, allowing you to do on-premise inference. . I also wrote a Streamlit app to help visualize the predictions from the model, allowing you to inspect your model performance. As I’ve built TrashNet, this has been especially useful in letting me know what data my model needs more learning on. The webapp is on course to be deployed internally at CleanRobotics by end of August 2020. . For a writeup on the project, click here. . For info on setting up TrashNet and other tutorials, head to the documentation here. . A 7-week intensive fellowship where you work on a real world project. Find out more here. &#8617; . |",
          "url": "https://jsaurabh.dev/trashnet/",
          "relUrl": "/trashnet/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jsaurabh.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}