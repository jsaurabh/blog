{
  
    
        "post0": {
            "title": "Ml Pipelines Ii",
            "content": "Machine Learning Pipelines - Part II . In Part I, we went ahead and wrote a rudimentary version of the pipeline we’ll be using for the webapp. . . It&#39;s not a pipeline In this post, we’ll focus on acquiring an initial dataset and extract and generate features. . Writing Better Questions . Following along with the book, we want to build an editor that lets its users write better questions. Before we go and build a model, the first step is playing around with the data. That begs the question, what kind of dataset should we be looking at? . Some good places to find datasets are Kaggle, the UCI Machine Learning Repository as well as the AWS Open Data registry. . For our use case, we’ll go ahead with StackExchange dump specifically the Writers dump here . Each of these dumps is an XML file with the headers and attributes containing the actual info we need. We need to extract raw text, and this is where we’ll write a basic pipeline to get the data we need. .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "relUrl": "/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Ml Pipelines",
            "content": "Machine Learning Pipelines - Part I . I recently started reading Building Machine Learning Powered Applications by Emmanuel Ammeisen. What little I’ve read so far has been excellent, and I love the approach of building a real application as we walk through the book. It’s just what I’ve been looking for and I can’t wait to apply what I learn to my projects and try to extend them from notebooks to production ready web apps, however miniature my operating scale is. . For now, this post is just me walking through the exact use case as the book(build an assistant to help users write better questions). Once I’m done with that, I plan to go back and follow the steps in the book for a real application of my own. What that is, I don’t know yet but for now - let’s dive into pipelines. . Pipelines . What are pipelines? In software engineering terms, pipelines are a series of processes on inputs in a sequential manner ie. output of one becomes the input to the other. For machine learning, this usually reduces to the data transformation and pre-processing that needs to be done before the data is suitable to be fed into an architecture. As ML has exploded, so has the research and time spent on developing scalable infrastructure. The field is in a constant state of flux, with new libraries and open source tools being released rapidly. Microsoft Azure has a nice overview on the different kinds of workflows involved in an ML pipeline [here].(https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines) . Our use case . For now, let’s continue with a simple pipeline of our own. We’ll assume that our model is already trained and focus on the inference/serving pipeline. An inference pipeline takes in the user input, processes it, passes it to the model and presents the user with the results. . Accept user input . We can build a simple web app with a textbox to allow the user to enter text input. For this, we’ll look into fastAPI which as the name suggests, is a web framework with high performance. For more info on fastAPI, the documentation has an excellent tutorial. . Let’s start by defining the .html for the text input web app -:p . &lt;html&gt; &lt;body&gt; &lt;form name=&quot;form1&quot; method=&quot;get&quot; action=&quot;/input&quot; enctype=&quot;application/json&quot;&gt; &lt;div&gt; &lt;h1&gt;Write better questions&lt;/h1&gt; &lt;input id=&quot;search&quot; name=&quot;search&quot; type=&quot;text&quot; /&gt; &lt;br&gt;&lt;br&gt; &lt;div&gt; &lt;input type=&quot;submit&quot; value=&quot;Search&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; &lt;/html&gt; . This will give us a simple text input box, followed by a search button. It should look as follows: . Text input box Next up, we’ll define the logic needed to handle the incoming input using fastAPI. I know it’s overkill but it’s a chance for me to learn and dive deep into fastAPI which is something I’ve wanted to get into for a while. . from fastapi import FastAPI from starlette.templating import Jinja2Templates app = FastAPI() templates = Jinja2Templates(directory=&#39;templates&#39;) @app.route(&quot;/&quot;) def home(request): context = {&quot;request&quot;: request} return templates.TemplateResponse(&quot;index.html&quot;, context) @app.get(&quot;/input/&quot;) def read_input(search: str): return search . The lines of code above are straight from the fastAPI documentation. The index.html template that we defined above gets served at the root address (in this case localhost on port 8000) and the second function takes in the entered text as a query parameter and returns it. Trivial yes, but it defines the workflow that we are going to be using. . Process User Input . Now that we’ve accepted in a string, let’s go and process it. Pre-processing steps for textual data usually involve removing puncutation, converting to lowercase, removing stop words, white spaces etc. There’s a lot of room here, and the preprocessing involved will be dictated by the task and data at hand. Let’s dive into the pre-processing. . def lower(text: str) -&gt; str: return text.lower() def remove_punctuation(text: str) -&gt; str: return text.translate(str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation)) def sanitize_ascii(text: str) -&gt; str: return text.encode(encoding=&quot;ascii&quot;, errors=&quot;ignore&quot;).decode() . Starting out, we can assume that this level of pre-processing is enough. Later on, we’ll come back and go through all the steps required for serving such an app in production. . Tokenize . For tokenizing our input, we’ll leverage the spaCy tokenizer. We’ll also need to download an English language pretrained model to actually give us the tokens we need as the library by itself does not come with one built-in. You can refer the documentation here . import spacy as sp spacy = sp.load(&quot;en_core_web_sm&quot;) def tokenize(text: str) -&gt; List[str]: return [token.text for token in spacy(text)] . Feature Generation . Now that we’ve defined our pre-processing strategy, the next step is to generate features. Features, as the name suggests are indicators that let us learn something meaningful about the data. . We’ll focus on the Flesch reading-ease score . We assume that sentences only end with periods(.) and the input is in the English language. Let’s compute the individual stats for the Flesch reading-ease score: . def count_syllables(text: str) -&gt; int: ### https://codereview.stackexchange.com/a/224180 return len( re.findall(&#39;(?!e$)[aeiouy]+&#39;, text, re.I) + re.findall(&#39;^[^aeiouy]*e$&#39;, text, re.I) ) def count_words(text: str) -&gt; int: return len(re.findall(r&#39; w+&#39;, text)) def count_sentences(text: str) -&gt; int: return len(re.split(r&#39;.+&#39;, text)) . The Flesch score can then be easily computed as follows: . def flesch_score(text: str) -&gt; float: total_syllables = count_syllables(text) total_words = count_words(text) total_sentences = count_sentences(text) return 206.835 - 1.015 * float(total_words)/total_sentences - 84.6 * (float(total_syllables)/total_words) . That’s it for this post. I’ll pick up with Feature Extraction for the next post. . Any questions/comments or suggestions, please let me know in the comments below! .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/02/26/ml-pipelines.html",
            "relUrl": "/ml/mlpa/2020/02/26/ml-pipelines.html",
            "date": " • Feb 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Computer Science Masters grad from SUNY Buffalo, with a specialization in Artificial Intelligence. During my time at UB, I worked on autonomous vehicles for the iCAVE2 project as well as swarm robotics for the DARPA OFFSET Program grant. . I also ran the Deep Learning student community at UB, where we held workshops, tutorials, hackathons and talks on topics as diverse as language modeling to graph neural networks. . I want to work on production machine learning software systems. .",
          "url": "https://jsaurabh.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}