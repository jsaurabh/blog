{
  
    
        "post0": {
            "title": "Localization for Autonomous Vehicles",
            "content": "Localization . Localization, as you have probably guessed, is the process of locating oneself, usually in the confines of an environment. In the context of autonomous vehicles, localization is an extremely important step. It is what enables self driving cars to function properly and make proper turns, rather than crashing head first into a traffic signal or a tree or any other kind of obstacle that may be in its path. . As the name implies, to localize is to know where you are. Historically, Global Positioning Systems(GPS) has been used for this very purpose. However, as important and useful GPS is, it&#39;s simply not accurate enough to be useful in critical systems such as self driving cars. It is essential that a self driving car be accurate in its measurements and have confidence in that accuracy, or the alternative is tons of crashes and accidents. . GPS has upto 2-10 meters of error. That&#39;s not ideal, and the error needs to be better, in cm rather than meters That&#39;s the question that the localization process tries to answer. Let&#39;s get into the intution of how that actually happens, and then we&#39;ll implement a set of functions in python that are essentially how localization for self driving car works. . &gt; Important: The type of localization I&#39;m writing about needs a good high definition map of the environment that the car is going to be operating in. Simultaneous Localization and Mapping(SLAM) is another type of localization that doesn&#39;t suffer from the same constraint . Intuition . Localization really is about continuous sensing and movement. You start out with a prior belief, which represents your view of what the environment looks like. On the basis of the belief, we sense the environment, update our belief based on the sensing results and make movements that correspond to the new information we learned from sensing. . In a perfect world, robot motion would be exact. However, it hardly ever is. Sensing itself is not perfect, and there&#39;s always going to be some kind of error involved, either in your movements or in the precision of your sensing. But intuitively, accounting for imperfect sensing and motion, it follows that we gain information when we sense(that information may not be highly accurate, but it&#39;s a gain). And similarly, when we move with inexact motion, we lose information. That&#39;s what localization really is, a continuous cycle of sensing and movement. And the imperfectness of the motion and sensing is what makes it a hard problem to solve. . Let&#39;s try to understand, using code, these two processes. . Sensing . Sensing takes in a belief(or a prior distribution) for the environment, and then updates that belief after sensing the environment. Say you have a bunch of buckets, either green or red in a uniform probability distribution. You go on and sense, say red buckets. It follows from probability theory that the belief for the red buckets should go up, and correspondingly the belief for the green buckets should go down. . Let&#39;s implement this in code. . #collapse-show import random n_buckets = 5 buckets = [random.choice([&#39;green&#39;, &#39;red&#39;]) for _ in range(n_buckets)] p = [1./n_buckets] * n_buckets # uniform probability distribution print(buckets) print(p) . . [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;] [0.2, 0.2, 0.2, 0.2, 0.2] . This defines the environment and our prior belief. Now, let&#39;s see what happens when we sense. Say we sense a red bucket. How does this affect my prior belief over different buckets? . The probabilities for the red buckets should go up, and the ones for green buckets should go down. Any bucket for which the color and our sensing is correct, we multipy it by a large weight, and a smaller weight for which it&#39;s incorrect. . #collapse-show pHit = 0.6 #for correct measurements pMiss = 0.2 #for incorrect measurements for i in range(n_buckets): if buckets[i] == &#39;green&#39;: p[i] *= pMiss else: p[i] *= pHit print(p) . . [0.12, 0.12, 0.04000000000000001, 0.04000000000000001, 0.04000000000000001] . This represents our new distribution, also called a posterior probability distribution. However, as you can see it&#39;s not a valid probability distribution as it doesn&#39;t sum to 1. To fix that, we&#39;ll normalize it . #collapse-show s = sum(p) for i in range(n_buckets): p[i] /= s print(p) . . [0.3333333333333332, 0.3333333333333332, 0.1111111111111111, 0.1111111111111111, 0.1111111111111111] . As you can see, this is a valid distribution. This is what sensing does. If you compare our posterior distribution with our prior belief, we can see that we have more information and have a greater idea about the environment for the given task(namely sensing red buckets). This is the information gain described earlier. . Let&#39;s refactor everything into one function. . def sense(p, t): res = [] for idx, _ in enumerate(p): hit = (buckets[idx] == t) res.append(p[idx] * (hit * pHit + (1-hit) * pMiss)) s = sum(res) for i in range(len(res)): res[i] /= s return res n_buckets = 5 buckets = [random.choice([&#39;green&#39;, &#39;red&#39;]) for _ in range(n_buckets)] p = [1./n_buckets] * n_buckets #prior print(buckets) print(&quot;Prior distribution is: {0}&quot;.format(p)) target = &#39;red&#39; p = sense(p, target) #posterior print(&quot;Posterior distribution is: {0}&quot;.format(p)) . [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;red&#39;] Prior distribution is: [0.2, 0.2, 0.2, 0.2, 0.2] Posterior distribution is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] . Movement . Movement is the second part of the localization equation. We&#39;ll only deal with inexact motion, exact motion is a trivial case and a rare scenarior in real world performance. . Assuming the world is cyclic, that is the rightmost bucket wraps around as the new leftmost bucket as you move them around. Inexact motion exists when there is uncertainty about the accuracy of the motion of a robot. . Let&#39;s define the parameters for the inexact motion. Say the probabilties are as follows: . Exact Motion : $P(X_{i+t} mid X_{i})$ = 0.8 | Undershooting: $P(X_{i+t-1} mid X_{i})$ = 0.1 | Overshooting : $P(X_{i+t+1} mid X_{i})$ = 0.1 where t is the target | . What this means is that we have an 80% chance of an exact motion and a 10% chance of either undershooting or overshooting our target. Assume that overshooting or undershooting happens in incrmenents(decrements) of 1. . Let&#39;s define inexact motion, accounting for the probabilities defined above. . &gt; Note: For uniform distributions, inexact motion has no effect on the posterior distribution. Uniform distributions are the state of least information. As everything is equally likely, we know nothing extra. . def move(p, unit): q = [] for i in range(len(p)): s = pExact * p[(i-unit% len(p))] s += pOvershoot * p[(i+1-unit) % len(p)] s += pUndershoot * p[(i-1-unit) % len(p)] q.append(s) return q pExact = 0.8 pOvershoot, pUndershoot = 0.1, 0.1 print(&quot;Environment is: {0}&quot;.format(buckets)) print(&quot;Prior distribution is: {0}&quot;.format(p)) print(&quot;Distribution after movement is: {0}&quot;.format(move(p, 1))) . Environment is: [&#39;red&#39;, &#39;red&#39;, &#39;green&#39;, &#39;green&#39;, &#39;red&#39;] Prior distribution is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] Distribution after movement is: [0.2545454545454545, 0.2727272727272727, 0.2545454545454545, 0.1090909090909091, 0.1090909090909091] . It&#39;s not easily seen, but given our target is red buckets, the distribution after movements suggests that we now know less about the presence of red buckets in the environment. What would happen if we were to move again, say 2 times? . #collapse-show print(&quot;Prior distribution(after sensing) is: {0}&quot;.format(p)) for i in range(2): p = move(p, 1) print(&quot;Posterior distribution(after movement) is: {0}&quot;.format(p)) . . Prior distribution(after sensing) is: [0.2727272727272727, 0.2727272727272727, 0.09090909090909091, 0.09090909090909091, 0.2727272727272727] Posterior distribution(after movement) is: [0.12363636363636364, 0.24181818181818182, 0.26909090909090905, 0.24181818181818182, 0.12363636363636364] . The uncertainty keeps increasing, and the amount of information gain keeps going down(wrt to the target). This is the entire localization loop, a continuous iteration of sense and move. You take an initial belief, use it to sense and then move accordingly. Every iteration of this loop(as we&#39;ve just shown) cumulatively causes information loss, largely because of the inexact motion of robots. . Every time we sense, the probability distribution is more focused ie. it learns more and after every iteration of move, the distribution is a bit more spread out because of information loss. . Formalizing Localization . We looked at a very simple example above. While it may not look like we&#39;ve implemented localization, it does implement the essence of how localization works. In the real world, the green and red buckets would be substituted by lane markers, trees, signposts etc. And the measurements obtained from sensing(say a camera) would be the color of those individual objects. This same approach can be used to work on image data, as long as we can make the proper associations. . Sensing . $P(X mid Z) = frac {P(Z mid X) * P(X)}{P(Z)}$ . The above equation defines the Bayes rule, which incorporates prior knowledge to find the probability of an event occuring. . In terms of measurements(sensing), consider X to be one of the grids in a map and Z to be the measurement update. So, the equation described above intends to calculate the probability of the location after taking the measurement into consideration . Here, $P(X)$ is the prior distribution and $P(X mid Z) is the probability of a colored bucket for every possible location in the environment. The product of these will correspond to the non-normalized probability distribution from earlier. . It then follows that $P(Z)$ is the sum of such probability distributions, over all grid cells in the environment leading to a normalized posterior distribution. What $P(Z)$ is really doing is assigning each cell a probability, irrespective of which cell it is. . 1. Referenced from the Localization module in the Udacity Self Driving Cars Nanodegree program↩ .",
            "url": "https://jsaurabh.dev/robotics/sdc/2020/04/21/localization.html",
            "relUrl": "/robotics/sdc/2020/04/21/localization.html",
            "date": " • Apr 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning Pipelines - Part II",
            "content": "Machine Learning Pipelines - Part II . In Part I, we went ahead and wrote a rudimentary version of the pipeline we’ll be using for the webapp. . . It&#39;s not a pipeline In this post, we’ll focus on acquiring an initial dataset and extract and generate features. . Writing Better Questions . Following along with the book, we want to build an editor that lets its users write better questions. Before we go and build a model, the first step is playing around with the data. That begs the question, what kind of dataset should we be looking at? . Some good places to find datasets are Kaggle, the UCI Machine Learning Repository as well as the AWS Open Data registry. . For our use case, we’ll go ahead with StackExchange dump specifically the Writers dump here . Each of these dumps is an XML file with the headers and attributes containing the actual info we need. We need to extract raw text, and this is where we’ll write a basic pipeline to get the data we need. .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "relUrl": "/ml/mlpa/2020/03/04/ml-pipelines-ii.html",
            "date": " • Mar 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Machine Learning Pipelines - Part I",
            "content": "Machine Learning Pipelines - Part I . I recently started reading Building Machine Learning Powered Applications by Emmanuel Ammeisen. What little I’ve read so far has been excellent, and I love the approach of building a real application as we walk through the book. It’s just what I’ve been looking for and I can’t wait to apply what I learn to my projects and try to extend them from notebooks to production ready web apps, however miniature my operating scale is. . For now, this post is just me walking through the exact use case as the book(build an assistant to help users write better questions). Once I’m done with that, I plan to go back and follow the steps in the book for a real application of my own. What that is, I don’t know yet but for now - let’s dive into pipelines. . Pipelines . What are pipelines? In software engineering terms, pipelines are a series of processes on inputs in a sequential manner ie. output of one becomes the input to the other. For machine learning, this usually reduces to the data transformation and pre-processing that needs to be done before the data is suitable to be fed into an architecture. As ML has exploded, so has the research and time spent on developing scalable infrastructure. The field is in a constant state of flux, with new libraries and open source tools being released rapidly. Microsoft Azure has a nice overview on the different kinds of workflows involved in an ML pipeline [here].(https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines) . Our use case . For now, let’s continue with a simple pipeline of our own. We’ll assume that our model is already trained and focus on the inference/serving pipeline. An inference pipeline takes in the user input, processes it, passes it to the model and presents the user with the results. . Accept user input . We can build a simple web app with a textbox to allow the user to enter text input. For this, we’ll look into fastAPI which as the name suggests, is a web framework with high performance. For more info on fastAPI, the documentation has an excellent tutorial. . Let’s start by defining the .html for the text input web app -:p . &lt;html&gt; &lt;body&gt; &lt;form name=&quot;form1&quot; method=&quot;get&quot; action=&quot;/input&quot; enctype=&quot;application/json&quot;&gt; &lt;div&gt; &lt;h1&gt;Write better questions&lt;/h1&gt; &lt;input id=&quot;search&quot; name=&quot;search&quot; type=&quot;text&quot; /&gt; &lt;br&gt;&lt;br&gt; &lt;div&gt; &lt;input type=&quot;submit&quot; value=&quot;Search&quot;&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; &lt;/html&gt; . This will give us a simple text input box, followed by a search button. It should look as follows: . Text input box Next up, we’ll define the logic needed to handle the incoming input using fastAPI. I know it’s overkill but it’s a chance for me to learn and dive deep into fastAPI which is something I’ve wanted to get into for a while. . from fastapi import FastAPI from starlette.templating import Jinja2Templates app = FastAPI() templates = Jinja2Templates(directory=&#39;templates&#39;) @app.route(&quot;/&quot;) def home(request): context = {&quot;request&quot;: request} return templates.TemplateResponse(&quot;index.html&quot;, context) @app.get(&quot;/input/&quot;) def read_input(search: str): return search . The lines of code above are straight from the fastAPI documentation. The index.html template that we defined above gets served at the root address (in this case localhost on port 8000) and the second function takes in the entered text as a query parameter and returns it. Trivial yes, but it defines the workflow that we are going to be using. . Process User Input . Now that we’ve accepted in a string, let’s go and process it. Pre-processing steps for textual data usually involve removing puncutation, converting to lowercase, removing stop words, white spaces etc. There’s a lot of room here, and the preprocessing involved will be dictated by the task and data at hand. Let’s dive into the pre-processing. . def lower(text: str) -&gt; str: return text.lower() def remove_punctuation(text: str) -&gt; str: return text.translate(str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation)) def sanitize_ascii(text: str) -&gt; str: return text.encode(encoding=&quot;ascii&quot;, errors=&quot;ignore&quot;).decode() . Starting out, we can assume that this level of pre-processing is enough. Later on, we’ll come back and go through all the steps required for serving such an app in production. . Tokenize . For tokenizing our input, we’ll leverage the spaCy tokenizer. We’ll also need to download an English language pretrained model to actually give us the tokens we need as the library by itself does not come with one built-in. You can refer the documentation here . import spacy as sp spacy = sp.load(&quot;en_core_web_sm&quot;) def tokenize(text: str) -&gt; List[str]: return [token.text for token in spacy(text)] . Feature Generation . Now that we’ve defined our pre-processing strategy, the next step is to generate features. Features, as the name suggests are indicators that let us learn something meaningful about the data. . We’ll focus on the Flesch reading-ease score . We assume that sentences only end with periods(.) and the input is in the English language. Let’s compute the individual stats for the Flesch reading-ease score: . def count_syllables(text: str) -&gt; int: ### https://codereview.stackexchange.com/a/224180 return len( re.findall(&#39;(?!e$)[aeiouy]+&#39;, text, re.I) + re.findall(&#39;^[^aeiouy]*e$&#39;, text, re.I) ) def count_words(text: str) -&gt; int: return len(re.findall(r&#39; w+&#39;, text)) def count_sentences(text: str) -&gt; int: return len(re.split(r&#39;.+&#39;, text)) . The Flesch score can then be easily computed as follows: . def flesch_score(text: str) -&gt; float: total_syllables = count_syllables(text) total_words = count_words(text) total_sentences = count_sentences(text) return 206.835 - 1.015 * float(total_words)/total_sentences - 84.6 * (float(total_syllables)/total_words) . That’s it for this post. I’ll pick up with Feature Extraction for the next post. . Any questions/comments or suggestions, please let me know in the comments below! .",
            "url": "https://jsaurabh.dev/ml/mlpa/2020/02/26/ml-pipelines.html",
            "relUrl": "/ml/mlpa/2020/02/26/ml-pipelines.html",
            "date": " • Feb 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jsaurabh.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jsaurabh.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}